import configparserfrom datetime import datetimeimport osimport findsparkfindspark.init()from pyspark.sql import SparkSessionfrom pyspark.sql.functions import udf, col, monotonically_increasing_idfrom pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, dayofweekfrom pyspark.sql.types import *config = configparser.ConfigParser()config.read('dl.cfg')# This setting is for local test purpose# os.environ['JAVA_HOME'] = '/Library/Java/JavaVirtualMachines/jdk1.8.0_271.jdk/Contents/Home'# os.environ['CLASS_PATH'] = '$PATH:$JAVA_HOME/lib'os.environ['AWS_ACCESS_KEY_ID'] = config.get('AWS', 'AWS_ACCESS_KEY_ID')os.environ['AWS_SECRET_ACCESS_KEY'] = config.get('AWS', 'AWS_ACCESS_KEY_ID')def create_spark_session():    spark = SparkSession \        .builder \        .config('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:2.7.0') \        .getOrCreate()    return sparkdef process_song_data(spark, input_data, output_data):    """        Process the songs data files and output songs table and artist table data from it.    :param spark: sparkSession object    :param input_data: input file path    :param output_data: output file path    """    # get filepath to song data file    song_data_path = input_data + '/song_data/*/*/*/*'    # read song data file    df = spark.read.json(song_data_path, mode='PERMISSIVE', columnNameOfCorruptRecord='corrupt_record') \        .dropDuplicates()    df.printSchema()    # # extract columns to create songs table    songs_df = df.select('song_id', 'title', 'artist_id', 'year', 'duration').dropDuplicates()    # # write songs table to parquet files partitioned by year and artist    songs_df.write.mode('overwrite') \        .parquet(output_data + '/songs', partitionBy=['year', 'artist_id'])    # # extract columns to create artists table    artists_df = df \        .select('artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude') \        .withColumnRenamed('artist_name', 'name') \        .withColumnRenamed('artist_location', 'location') \        .withColumnRenamed('artist_latitude', 'latitude') \        .withColumnRenamed('artist_location', 'longitude') \        .dropDuplicates()    # # write artists table to parquet files    artists_df.write.mode('overwrite').parquet(output_data + '/artists')def process_log_data(spark, input_data, output_data):    """            Process the event log file and extract data for table time, users and songplays from it.    :param spark: sparkSession object    :param input_data: input file path    :param output_data: output file path    """    # get filepath to log data file    log_data = input_data + '/log_data'    # read log data file    df = spark.read.json(log_data)    df.printSchema()    # filter by actions for song plays    df = df.filter(df.page == 'NextSong')    # extract columns for users table    user_table = df.select('userId', 'firstName', 'lastName', 'gender', 'level')    # # write users table to parquet files    user_table.write.mode('overwrite').parquet(output_data + '/users')    # # create timestamp column from original timestamp column    get_timestamp = udf(lambda x: datetime.utcfromtimestamp(int(x) / 1000), TimestampType())    df = df.withColumn('start_time', get_timestamp('ts'))    # # extract columns to create time table    time_table = df \        .withColumn('hour', hour('start_time')) \        .withColumn('day', dayofmonth('start_time')) \        .withColumn('week', weekofyear('start_time')) \        .withColumn('month', month('start_time')) \        .withColumn('year', year('start_time')) \        .withColumn('weekday', dayofweek('start_time')) \        .select('ts', 'start_time', 'hour', 'day', 'week', 'month', 'year', 'weekday') \        .drop_duplicates()    # # write time table to parquet files partitioned by year and month    time_table.write.mode('overwrite').parquet(output_data + '/time', partitionBy=['year', 'month'])    time_table.cache()    # # read in song data to use for songplays table    song_df = spark.read \        .option('basePath', os.path.join(output_data, 'songs')) \        .parquet(output_data + '/songs/*/*/*')    # # extract columns from joined song and log datasets to create songplays table    songplays_table = df.join(song_df, df.song == song_df.title, how='inner') \        .select(monotonically_increasing_id().alias('songplay_id'), 'start_time', col('userId').alias('user_id'),                'level', 'song_id', 'artist_id', col('sessionId').alias('session_id'), 'location',                col('userAgent').alias('user_agent'))    songplays_table = songplays_table \        .join(time_table, songplays_table.start_time == time_table.start_time) \        .select('songplay_id', songplays_table.start_time, 'user_id', 'level', 'song_id', 'artist_id', 'session_id',                'location', 'user_agent', 'year', 'month')    # # write songplays table to parquet files partitioned by year and month    songplays_table.write.mode('overwrite').parquet(output_data + '/songplays', partitionBy=['year', 'month'])def main():    spark = create_spark_session()    input_data = 's3a://udacity-dend/'    # input_data = 'data'    output_data = 'data/output'    process_song_data(spark, input_data, output_data)    process_log_data(spark, input_data, output_data)    spark.stop()if __name__ == '__main__':    main()